{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058ccb0d",
   "metadata": {},
   "source": [
    "# Spam Classifier \n",
    "\n",
    "This notebook demonstrates building a spam classifier using the SpamAssassin dataset. It reuses the train and test sets previously split and guides you through data loading, preprocessing, feature extraction, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c6efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc8ab4",
   "metadata": {},
   "source": [
    "## Load Pre-Split Train and Test Sets\n",
    "using the train and test folders created previously for each category (easy_ham, hard_ham, spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b43561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample files in 20021010_easy_ham/easy_ham/train:\n",
      "['0003.acfc5ad94bbd27118a0d8685d18c89dd', '0004.e8d5727378ddde5c3be181df593f1712', '0005.8c3b9e9c0f3f183ddaf7592a11b99957', '0006.ee8b0dba12856155222be180ba122058', '0007.c75188382f64b090022fa3b095b020b0']\n",
      "Sample files in 20030228_hard_ham/hard_ham/train:\n",
      "['00001.7c7d6921e671bbe18ebb5f893cd9bb35', '00003.268fd170a3fc73bee2739d8204856a53', '00004.68819fc91d34c82433074d7bd3127dcc', '00005.34bcaad58ad5f598f5d6af8cfa0c0465', '00006.3409dec8ca4fcf2d6e0582554473b5c9']\n",
      "Sample files in 20030228_spam/spam/train:\n",
      "['00001.7848dde101aa985090474a91ec93fcf0', '00003.2ee33bc6eacdb11f38d052c44819ba6c', '00004.eac8de8d759b7e74154f142194282724', '00005.57696a39d7d84318ce497886896bf90d', '00006.5ab5620d3d7c6c0db76234556a16f6c1']\n",
      "Sample files in 20021010_easy_ham/easy_ham/test:\n",
      "['0001.ea7e79d3153e7469e7a9c3e0af6a357e', '0002.b3120c4bcbf3101e661161ee7efcb8bf', '0011.07b11073b53634cff892a7988289a72e', '0015.a9ff8d7550759f6ab62cc200bdf156e7', '0018.ba70ecbeea6f427b951067f34e23bae6']\n",
      "Sample files in 20030228_hard_ham/hard_ham/test:\n",
      "['00002.ca96f74042d05c1a1d29ca30467cfcd5', '00017.840244edb8cc88aba7129296ea536212', '00019.e35a7a6a1a6bdd0d2e164db2f6a0e4ef', '00020.eca3ca2c144cd6ceaf677f3d8b7b0eed', '00031.2ca10bdfae710ae978fe69ac84d9e98c']\n",
      "Sample files in 20030228_spam/spam/test:\n",
      "['00002.d94f1b97e48ed3b553b3508d116e6a09', '00017.1a938ecddd047b93cbd7ed92c241e6d1', '00019.bbc97ad616ffd06e93ce0f821ca8c381', '00024.6b5437b14d403176c3f046c871b5b52f', '00026.da18dbed27ae933172f7a70f860c6ad0']\n"
     ]
    }
   ],
   "source": [
    "# Define paths to train and test folders\n",
    "train_folders = [\n",
    "    '20021010_easy_ham/easy_ham/train',\n",
    "    '20030228_hard_ham/hard_ham/train',\n",
    "    '20030228_spam/spam/train'\n",
    "]\n",
    "test_folders = [\n",
    "    '20021010_easy_ham/easy_ham/test',\n",
    "    '20030228_hard_ham/hard_ham/test',\n",
    "    '20030228_spam/spam/test'\n",
    "]\n",
    "\n",
    "# List a few files from each folder as a check\n",
    "for folder in train_folders + test_folders:\n",
    "    print(f\"Sample files in {folder}:\")\n",
    "    print(os.listdir(folder)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e92ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 63977\n",
      "Train set: (2640, 63977), Test set: (663, 63977)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing and vectorization functions\n",
    "REMOVE_HEADERS = True\n",
    "TO_LOWER = True\n",
    "REMOVE_PUNCT = True\n",
    "REPLACE_URLS = True\n",
    "REPLACE_NUMBERS = True\n",
    "BINARY_FEATURES = True  # True: presence/absence, False: word counts\n",
    "\n",
    "def preprocess(text):\n",
    "    if REMOVE_HEADERS:\n",
    "        parts = text.split('\\n\\n', 1)\n",
    "        text = parts[1] if len(parts) > 1 else text\n",
    "    if TO_LOWER:\n",
    "        text = text.lower()\n",
    "    if REPLACE_URLS:\n",
    "        text = re.sub(r'http[s]?://\\S+', 'URL', text)\n",
    "    if REPLACE_NUMBERS:\n",
    "        text = re.sub(r'\\b\\d+\\b', 'NUMBER', text)\n",
    "    if REMOVE_PUNCT:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "def get_files(folder):\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "def load_emails(folder, label):\n",
    "    emails = []\n",
    "    labels = []\n",
    "    for fname in get_files(folder):\n",
    "        with open(fname, encoding='latin1') as f:\n",
    "            text = f.read()\n",
    "            words = preprocess(text)\n",
    "            emails.append(words)\n",
    "            labels.append(label)\n",
    "    return emails, labels\n",
    "\n",
    "# Load all emails and labels\n",
    "train_emails, train_labels = [], []\n",
    "test_emails, test_labels = [], []\n",
    "\n",
    "for i, folder in enumerate(train_folders):\n",
    "    label = 0 if i < 2 else 1  # easy_ham/hard_ham=0, spam=1\n",
    "    emails, labels = load_emails(folder, label)\n",
    "    train_emails.extend(emails)\n",
    "    train_labels.extend(labels)\n",
    "for i, folder in enumerate(test_folders):\n",
    "    label = 0 if i < 2 else 1\n",
    "    emails, labels = load_emails(folder, label)\n",
    "    test_emails.extend(emails)\n",
    "    test_labels.extend(labels)\n",
    "\n",
    "# Build vocabulary from training set\n",
    "vocab_counter = Counter()\n",
    "for words in train_emails:\n",
    "    vocab_counter.update(set(words) if BINARY_FEATURES else words)\n",
    "vocab = sorted(vocab_counter)\n",
    "word_idx = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Vectorize emails\n",
    "def vectorize(emails):\n",
    "    X = np.zeros((len(emails), len(vocab)), dtype=int)\n",
    "    for i, words in enumerate(emails):\n",
    "        counts = Counter(words)\n",
    "        for word in counts:\n",
    "            if word in word_idx:\n",
    "                X[i, word_idx[word]] = 1 if BINARY_FEATURES else counts[word]\n",
    "    return X\n",
    "\n",
    "X_train = vectorize(train_emails)\n",
    "X_test = vectorize(test_emails)\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27057f36",
   "metadata": {},
   "source": [
    "## Training and Evaluating the Classifier\n",
    "We will train a Multinomial Naive Bayes classifier on the training data and evaluate its performance on the test set using accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46947551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9532428355957768\n",
      "Precision: 0.9861111111111112\n",
      "Recall: 0.7029702970297029\n",
      "F1 Score: 0.8208092485549133\n",
      "\n",
      "Confusion Matrix:\n",
      " [[561   1]\n",
      " [ 30  71]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.97       562\n",
      "        spam       0.99      0.70      0.82       101\n",
      "\n",
      "    accuracy                           0.95       663\n",
      "   macro avg       0.97      0.85      0.90       663\n",
      "weighted avg       0.95      0.95      0.95       663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"ham\", \"spam\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
